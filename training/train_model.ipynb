{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory-Optimized Chat Memory Model Training\n",
    "\n",
    "This notebook trains a chat summarization model using the Llama architecture with memory optimizations for Google Colab.\n",
    "\n",
    "## Setup Steps:\n",
    "1. Mount Google Drive\n",
    "2. Install dependencies\n",
    "3. Configure memory settings\n",
    "4. Train model with optimizations\n",
    "5. Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "%%capture\n",
    "!pip install torch transformers datasets accelerate bitsandbytes trl peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Mount Google Drive and setup directories\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create project directories\n",
    "!mkdir -p \"/content/drive/MyDrive/chat_memory/models\"\n",
    "!mkdir -p \"/content/drive/MyDrive/chat_memory/data\"\n",
    "!mkdir -p \"/content/drive/MyDrive/chat_memory/notebooks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Configure memory management\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "\n",
    "# Import required libraries\n",
    "import json\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Verify GPU Setup and Memory\n",
    "def print_gpu_memory():\n",
    "    \"\"\"Print GPU memory usage\"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"\\nGPU Memory Summary:\")\n",
    "    print(f\"Allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "    print(f\"Cached: {torch.cuda.memory_reserved()/1e9:.2f} GB\")\n",
    "    \n",
    "    !nvidia-smi\n",
    "\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"GPU Device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Configuration\n",
    "max_seq_length = 2048  # Reduced for memory optimization\n",
    "model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    texts = []\n",
    "    \n",
    "    for i in range(len(examples['messages'])):\n",
    "        # Format the messages\n",
    "        messages_text = \"\\n\".join([\n",
    "            f\"{msg['timestamp']} | {msg['content']}\"\n",
    "            for msg in examples['messages'][i]\n",
    "        ])\n",
    "        \n",
    "        # Create the instruction prompt\n",
    "        instruction = \"\"\"You are a chat summarization assistant. Given a conversation and its date range, create a concise yet comprehensive summary that captures the key points, emotional undertones, and progression of the relationship between participants.\"\"\"\n",
    "        \n",
    "        # Format the input with context\n",
    "        input_text = f\"\"\"Please summarize the following chat conversation that occurred between {examples['start_date'][i]} and {examples['end_date'][i]}.\n",
    "\n",
    "[START DATE]\n",
    "{examples['start_date'][i]}\n",
    "[END DATE]\n",
    "{examples['end_date'][i]}\n",
    "[CHAT MESSAGES]\n",
    "{messages_text}\"\"\"\n",
    "\n",
    "        # Create the full prompt with instruction format\n",
    "        prompt = f\"\"\"<s>[INST] {instruction}\n",
    "\n",
    "{input_text} [/INST]\n",
    "[SUMMARY]\n",
    "{examples['summary'][i]}</s>{tokenizer.eos_token}\"\"\"\n",
    "        \n",
    "        texts.append(prompt)\n",
    "        \n",
    "        # Clear memory periodically\n",
    "        if i % 10 == 0:\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "def load_and_prepare_data(file_path, chunk_size=50):\n",
    "    data = []\n",
    "    # Process file in chunks to reduce memory usage\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            data.append(json.loads(line))\n",
    "            if i % chunk_size == 0:\n",
    "                gc.collect()\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = Dataset.from_list(data)\n",
    "    \n",
    "    print(\"\\nSample data point before formatting:\")\n",
    "    print(json.dumps(data[0], indent=2))\n",
    "    \n",
    "    return dataset"
   ]
  }
 ]
}
